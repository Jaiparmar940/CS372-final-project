# Improved configuration for better training performance

# Environment configuration
env:
  arena_size: 10.0
  max_steps: 500
  dt: 0.1
  damping: 0.05  # Reduced damping for more responsive control
  goal_threshold: 0.5
  obstacle_prob: 0.3  # Start with fewer obstacles (curriculum learning)
  obstacle_radius_range: [0.5, 1.5]
  reward_distance_scale: 0.1  # Much smaller distance penalty
  reward_success: 100.0
  reward_collision: -20.0  # Reduced collision penalty
  reward_time: -0.01  # Reduced time penalty
  goal_range: [-3.0, 3.0]  # Start with closer goals (curriculum learning)

# Agent configuration
agent:
  lr_actor: 1.0e-4  # Slightly lower learning rate for stability
  lr_critic: 1.0e-4
  gamma: 0.99
  gae_lambda: 0.95
  clip_epsilon: 0.2
  value_coef: 0.5
  entropy_coef: 0.02  # Increased entropy for better exploration
  max_grad_norm: 0.5
  update_epochs: 4  # Fewer epochs per batch for faster updates
  actor_hidden_dims: [128, 128]  # Smaller network for faster training
  critic_hidden_dims: [128, 128]
  device: "cpu"  # Change to "cuda" if GPU available

# Training configuration
training:
  total_steps: 1000000
  rollout_length: 2048
  eval_frequency: 10000
  eval_episodes: 10
  save_frequency: 50000

# Logging configuration
logging:
  log_dir: "results"
  log_file: "training"

# Random seed
seed: 42

