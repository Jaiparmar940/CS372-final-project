# Attribution

## Sources and References

### Algorithms and Methods

- **Proximal Policy Optimization (PPO)**: 
  - Schulman, J., et al. "Proximal policy optimization algorithms." arXiv preprint arXiv:1707.06347 (2017).
  - Implementation based on standard PPO algorithm with clipped objective and GAE.

- **Generalized Advantage Estimation (GAE)**:
  - Schulman, J., et al. "High-dimensional continuous control using generalized advantage estimation." ICLR 2016.

- **Actor-Critic Architecture**:
  - Standard actor-critic framework for policy gradient methods.

### Libraries and Frameworks

- **PyTorch**: Deep learning framework for neural network implementation
  - https://pytorch.org/

- **Gymnasium**: Environment API standard (formerly OpenAI Gym)
  - https://gymnasium.farama.org/

- **NumPy**: Numerical computing library
  - https://numpy.org/

### Code Structure

The project structure follows standard RL research practices:
- Modular design separating environments, agents, training, and evaluation
- Configuration management using YAML files
- Standard Gymnasium API for environment interface

## AI Tool Usage

This project was developed with assistance from AI coding tools (Cursor/Auto) for:
- Code generation and structure
- Implementation of standard RL algorithms
- Documentation writing
- Project organization

All algorithmic implementations, design decisions, and project structure were guided by the project requirements and standard RL practices.

## License

[Specify license if applicable]

